{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################### Import libraries ###############################\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "################################## set device ##################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    \n",
    "print(\"============================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device).unsqueeze(0)  \n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device).unsqueeze(0)  \n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "        \n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## FEM Analysis ##################################\n",
    "\n",
    "import time\n",
    "import numpy as np                                                # for dense matrix ops\n",
    "import matplotlib.pyplot as plt                                   # for plotting\n",
    "import autograd, autograd.core, autograd.extend, autograd.tracer  # for adjoints\n",
    "import autograd.numpy as anp      \n",
    "import scipy, scipy.ndimage, scipy.sparse, scipy.sparse.linalg    # sparse matrices\n",
    "import nlopt   \n",
    "\n",
    "\n",
    "def causeway_bridge(width=128, height=128, density=0.08, deck_level=0.2):\n",
    "  \"\"\"A bridge supported by columns at a regular interval.\"\"\"\n",
    "  x_ix, y_ix = 0, 1\n",
    "  normals = np.zeros((width + 1, height + 1, 2))\n",
    "  normals[-1, -1, y_ix] = 1\n",
    "  normals[-1, :, x_ix] = 1\n",
    "  normals[0, :, x_ix] = 1\n",
    "\n",
    "  forces = np.zeros((width + 1, height + 1, 2))\n",
    "  forces[:, round(height * (1 - deck_level)), y_ix] = -1 / width\n",
    "  return normals, forces, density\n",
    "  \n",
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "    \n",
    "def get_args(normals, forces, density=0.4):  # Manage the problem setup parameters\n",
    "  width = normals.shape[0] - 1\n",
    "  height = normals.shape[1] - 1\n",
    "  fixdofs = np.flatnonzero(normals.ravel())\n",
    "  alldofs = np.arange(2 * (width + 1) * (height + 1))\n",
    "  freedofs = np.sort(list(set(alldofs) - set(fixdofs)))\n",
    "  params = {\n",
    "      # material properties\n",
    "      'young': 1, 'young_min': 1e-9, 'poisson': 0.3, 'g': 0,\n",
    "      # constraints\n",
    "      'density': density, 'xmin': 0.001, 'xmax': 1.0,\n",
    "      # input parameters\n",
    "      'nelx': width, 'nely': height, 'mask': 1, 'penal': 3.0, 'filter_width': 1,\n",
    "      'freedofs': freedofs, 'fixdofs': fixdofs, 'forces': forces.ravel(),\n",
    "      # optimization parameters\n",
    "      'opt_steps': 5, 'print_every': 20}\n",
    "  return ObjectView(params)\n",
    "\n",
    "def mbb_beam(width=80, height=25, density=0.4, y=1, x=0):  \n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    normals[-1, -1, y] = 1\n",
    "    normals[0, :, x] = 1\n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    forces[0, 0, y] = -1\n",
    "    return normals, forces, density\n",
    "#'''\n",
    "  \n",
    "\n",
    "def mbb_beam(width=6, height=6, density=0.4, y=1, x=0):  \n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    normals[-1, -1, y] = 1\n",
    "    normals[0, :, x] = 1\n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    forces[0, 0, y] = -1\n",
    "    forces[height//2, width, y] = -1\n",
    "    return normals, forces, density\n",
    "\n",
    "def mbb_beam2(width=6, height=6, density=0.4, y=1, x=0):  \n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    \n",
    "    normals[0, 0, :] = 1   \n",
    "    normals[0, width, :] = 1  \n",
    "    normals[0, width // 2, :] = 1  \n",
    "    \n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    \n",
    "    forces[-1, :, y] = -1 / width\n",
    "    \n",
    "    return normals, forces, density\n",
    "#'''\n",
    "def _get_solver(a_entries, a_indices, size, sym_pos):\n",
    "  # a is (usu.) symmetric positive; could solve 2x faster w/sksparse.cholmod.cholesky(a).solve_A\n",
    "  a = scipy.sparse.coo_matrix((a_entries, a_indices), shape=(size,)*2).tocsc()\n",
    "  return scipy.sparse.linalg.splu(a).solve\n",
    "\n",
    "@autograd.primitive\n",
    "def solve_coo(a_entries, a_indices, b, sym_pos=False):\n",
    "  solver = _get_solver(a_entries, a_indices, b.size, sym_pos)\n",
    "  return solver(b)\n",
    "\n",
    "def grad_solve_coo_entries(ans, a_entries, a_indices, b, sym_pos=False):\n",
    "  def jvp(grad_ans):\n",
    "    lambda_ = solve_coo(a_entries, a_indices if sym_pos else a_indices[::-1],\n",
    "                        grad_ans, sym_pos)\n",
    "    i, j = a_indices\n",
    "    return -lambda_[i] * ans[j]\n",
    "  return jvp\n",
    "\n",
    "autograd.extend.defvjp(solve_coo, grad_solve_coo_entries,\n",
    "                       lambda: print('err: gradient undefined'),\n",
    "                       lambda: print('err: gradient not implemented'))\n",
    "@autograd.extend.primitive\n",
    "def gaussian_filter(x, width): # 2D gaussian blur/filter\n",
    "  return scipy.ndimage.gaussian_filter(x, width, mode='reflect')\n",
    "\n",
    "def _gaussian_filter_vjp(ans, x, width): # gives the gradient of orig. function w.r.t. x\n",
    "  del ans, x  # unused\n",
    "  return lambda g: gaussian_filter(g, width)\n",
    "autograd.extend.defvjp(gaussian_filter, _gaussian_filter_vjp)\n",
    "def young_modulus(x, e_0, e_min, p=3):\n",
    "  return e_min + x ** p * (e_0 - e_min)\n",
    "\n",
    "def physical_density(x, args, volume_contraint=False, use_filter=False):\n",
    "  x = args.mask * x.reshape(args.nely, args.nelx)  # reshape from 1D to 2D\n",
    "  return gaussian_filter(x, args.filter_width) if use_filter else x  # maybe filter\n",
    "\n",
    "def mean_density(x, args, volume_contraint=False, use_filter=True):\n",
    "  return anp.mean(physical_density(x, args, volume_contraint, use_filter)) / anp.mean(args.mask)\n",
    "def objective_calc(x, args, volume_contraint=False, use_filter=True):\n",
    "  kwargs = dict(penal=args.penal, e_min=args.young_min, e_0=args.young)\n",
    "  x_phys = physical_density(x, args, volume_contraint=volume_contraint, use_filter=use_filter)\n",
    "  ke     = get_stiffness_matrix(args.young, args.poisson)  # stiffness matrix\n",
    "  u      = displace(x_phys, ke, args.forces, args.freedofs, args.fixdofs, **kwargs)\n",
    "  c      = compliance_calc(x_phys, u, ke, **kwargs)\n",
    "  #print(\"x_phys= \",  x_phys)\n",
    "  #print(\"compliance = \", c)\n",
    "  return c\n",
    "def compliance_calc(x_phys, u, ke, *, penal=3, e_min=1e-9, e_0=1):\n",
    "  nely, nelx = x_phys.shape\n",
    "  ely, elx = anp.meshgrid(range(nely), range(nelx))  # x, y coords for the index map\n",
    "\n",
    "  n1 = (nely+1)*(elx+0) + (ely+0)  # nodes\n",
    "  n2 = (nely+1)*(elx+1) + (ely+0)\n",
    "  n3 = (nely+1)*(elx+1) + (ely+1)\n",
    "  n4 = (nely+1)*(elx+0) + (ely+1)\n",
    "  all_ixs = anp.array([2*n1, 2*n1+1, 2*n2, 2*n2+1, 2*n3, 2*n3+1, 2*n4, 2*n4+1])\n",
    "  u_selected = u[all_ixs]  # select from u matrix\n",
    "\n",
    "  ke_u = anp.einsum('ij,jkl->ikl', ke, u_selected)  # compute x^penal * U.T @ ke @ U\n",
    "  ce = anp.einsum('ijk,ijk->jk', u_selected, ke_u)\n",
    "  C = young_modulus(x_phys, e_0, e_min, p=penal) * ce.T\n",
    "  return anp.sum(C)\n",
    "\n",
    "def get_stiffness_matrix(e, nu):  # e=young's modulus, nu=poisson coefficient\n",
    "  k = anp.array([1/2-nu/6, 1/8+nu/8, -1/4-nu/12, -1/8+3*nu/8,\n",
    "                -1/4+nu/12, -1/8-nu/8, nu/6, 1/8-3*nu/8])\n",
    "  return e/(1-nu**2)*anp.array([[k[0], k[1], k[2], k[3], k[4], k[5], k[6], k[7]],\n",
    "                               [k[1], k[0], k[7], k[6], k[5], k[4], k[3], k[2]],\n",
    "                               [k[2], k[7], k[0], k[5], k[6], k[3], k[4], k[1]],\n",
    "                               [k[3], k[6], k[5], k[0], k[7], k[2], k[1], k[4]],\n",
    "                               [k[4], k[5], k[6], k[7], k[0], k[1], k[2], k[3]],\n",
    "                               [k[5], k[4], k[3], k[2], k[1], k[0], k[7], k[6]],\n",
    "                               [k[6], k[3], k[4], k[1], k[2], k[7], k[0], k[5]],\n",
    "                               [k[7], k[2], k[1], k[4], k[3], k[6], k[5], k[0]]])\n",
    "def get_k(stiffness, ke):\n",
    "  # Constructs sparse stiffness matrix k (used in the displace fn)\n",
    "  # First, get position of the nodes of each element in the stiffness matrix\n",
    "  nely, nelx = stiffness.shape\n",
    "  ely, elx = anp.meshgrid(range(nely), range(nelx))  # x, y coords\n",
    "  ely, elx = ely.reshape(-1, 1), elx.reshape(-1, 1)\n",
    "\n",
    "  n1 = (nely+1)*(elx+0) + (ely+0)\n",
    "  n2 = (nely+1)*(elx+1) + (ely+0)\n",
    "  n3 = (nely+1)*(elx+1) + (ely+1)\n",
    "  n4 = (nely+1)*(elx+0) + (ely+1)\n",
    "  edof = anp.array([2*n1, 2*n1+1, 2*n2, 2*n2+1, 2*n3, 2*n3+1, 2*n4, 2*n4+1])\n",
    "  edof = edof.T[0]\n",
    "  x_list = anp.repeat(edof, 8)  # flat list pointer of each node in an element\n",
    "  y_list = anp.tile(edof, 8).flatten()  # flat list pointer of each node in elem\n",
    "\n",
    "  # make the global stiffness matrix K\n",
    "  kd = stiffness.T.reshape(nelx*nely, 1, 1)\n",
    "  value_list = (kd * anp.tile(ke, kd.shape)).flatten()\n",
    "  return value_list, y_list, x_list\n",
    "\n",
    "def displace(x_phys, ke, forces, freedofs, fixdofs, *, penal=3, e_min=1e-9, e_0=1):\n",
    "  # Displaces the load x using finite element techniques (solve_coo=most of runtime)\n",
    "  stiffness = young_modulus(x_phys, e_0, e_min, p=penal)\n",
    "  k_entries, k_ylist, k_xlist = get_k(stiffness, ke)\n",
    "\n",
    "  index_map, keep, indices = _get_dof_indices(freedofs, fixdofs, k_ylist, k_xlist)\n",
    "  \n",
    "  u_nonzero = solve_coo(k_entries[keep], indices, forces[freedofs], sym_pos=True)\n",
    "  u_values = anp.concatenate([u_nonzero, anp.zeros(len(fixdofs))])\n",
    "  return u_values[index_map]\n",
    "def _get_dof_indices(freedofs, fixdofs, k_xlist, k_ylist):\n",
    "  index_map = inverse_permutation(anp.concatenate([freedofs, fixdofs]))\n",
    "  keep = anp.isin(k_xlist, freedofs) & anp.isin(k_ylist, freedofs)\n",
    "  # Now we index an indexing array that is being indexed by the indices of k\n",
    "  i = index_map[k_ylist][keep]\n",
    "  j = index_map[k_xlist][keep]\n",
    "  return index_map, keep, anp.stack([i, j])\n",
    "\n",
    "def inverse_permutation(indices):  # reverses an index operation\n",
    "  inverse_perm = np.zeros(len(indices), dtype=anp.int64)\n",
    "  inverse_perm[indices] = np.arange(len(indices), dtype=anp.int64)\n",
    "  return inverse_perm\n",
    "def fast_stopt(args, x=None, verbose=True):\n",
    "  if x is None:\n",
    "    x = anp.ones((args.nely, args.nelx)) * args.density  # init mass\n",
    "\n",
    "  reshape = lambda x: x.reshape(args.nely, args.nelx)\n",
    "  objective_fn = lambda x: objective_calc(reshape(x), args) # don't enforce mass constraint here\n",
    "  constraint = lambda params: mean_density(reshape(params), args) - args.density\n",
    "\n",
    "  def wrap_autograd_func(func, losses=None, frames=None):\n",
    "    def wrapper(x, grad):\n",
    "      if grad.size > 0:\n",
    "        value, grad[:] = autograd.value_and_grad(func)(x)\n",
    "      else:\n",
    "        value = func(x)\n",
    "      if losses is not None:\n",
    "        losses.append(value)\n",
    "      if frames is not None:\n",
    "        frames.append(reshape(x).copy())\n",
    "        if verbose and len(frames) % args.print_every == 0:\n",
    "          print('step {}, loss {:.2e}, t={:.2f}s'.format(len(frames), value, time.time()-dt))\n",
    "      return value\n",
    "    return wrapper\n",
    "\n",
    "  losses, frames = [], [] ; dt = time.time()\n",
    "  print('Optimizing a problem with {} nodes'.format(len(args.forces)))\n",
    "  opt = nlopt.opt(nlopt.LD_MMA, x.size)\n",
    "  opt.set_lower_bounds(0.0) ; opt.set_upper_bounds(1.0)\n",
    "  opt.set_min_objective(wrap_autograd_func(objective_fn, losses, frames))\n",
    "\n",
    "  opt.add_inequality_constraint(wrap_autograd_func(constraint), 1e-8)\n",
    "  opt.set_maxeval(args.opt_steps + 1)\n",
    "  opt.optimize(x.flatten())\n",
    "  return np.array(losses), reshape(frames[-1]), np.array(frames), constraint(frames[-1])\n",
    "  #return objective(x, args), constraint(x)  #losses[-1]\n",
    "\n",
    "def optim( args, x=None, verbose = True):\n",
    "    if x is None:\n",
    "        x = anp.ones((args.nely, args.nelx)) * args.density  # init mass\n",
    "    reshape = lambda x: x.reshape(args.nely, args.nelx)\n",
    "    objective_fn = lambda x: objective_calc(reshape(x), args) # don't enforce mass constraint here\n",
    "    constraint = lambda params: mean_density(reshape(params), args) \n",
    "    return objective_calc(x, args), constraint(x)  #losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## GYM Enviroment ##################################\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "class BeamOptimizationEnv(gym.Env):\n",
    "    def __init__(self, width=6, height=6, density=0.4):\n",
    "        super(BeamOptimizationEnv, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.density = density\n",
    "        self.max_steps = self.width * self.height\n",
    "        self.action_space = gym.spaces.Discrete(self.width * self.height)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.width * self.height,), dtype=np.float32)\n",
    "        self.previous_constraint = 1.0\n",
    "        self.significant_improvement = 10\n",
    "        self.state = torch.ones((self.height, self.width), dtype=torch.float32, device=self.device) \n",
    "        self.visited = torch.zeros((self.height * self.width), dtype=bool, device=self.device)\n",
    "        self.visited_cells = set()\n",
    "        normals, forces, _ = mbb_beam(width, height, density)\n",
    "        self.normals = torch.tensor(normals, dtype=torch.float32, device=self.device)\n",
    "        self.forces = torch.tensor(forces, dtype=torch.float32, device=self.device)\n",
    "        self.args = get_args(self.normals.cpu().numpy(), self.forces.cpu().numpy(), density)\n",
    "        self.current_compliance = float('inf')\n",
    "        self.previous_compliance = 10\n",
    "        self.current_step = 0\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        self.state = torch.ones((self.width * self.height), dtype=torch.float32, device=self.device) \n",
    "        self.current_step = 0\n",
    "        self.visited = torch.zeros((self.width * self.height), dtype=bool)\n",
    "        self.visited_cells = set()\n",
    "        self.current_compliance = float('inf')\n",
    "        self.previous_compliance = 10\n",
    "        self.reward = 0\n",
    "        return self.state.cpu().numpy()  \n",
    "\n",
    "    def is_strongly_connected(self, state, threshold=0.8):\n",
    "        grid = state.cpu().numpy().reshape(self.height, self.width)\n",
    "        for i in range(self.height - 1):\n",
    "            for j in range(self.width - 1):\n",
    "                if grid[i, j] > threshold and grid[i + 1, j + 1] > threshold:\n",
    "                    if grid[i, j + 1] <= threshold and grid[i + 1, j] <= threshold:\n",
    "                        return False\n",
    "                if grid[i + 1, j] > threshold and grid[i, j + 1] > threshold:\n",
    "                    if grid[i, j] <= threshold and grid[i + 1, j + 1] <= threshold:\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def is_connected(self, state, threshold=0.8):\n",
    "        grid = state.cpu().numpy().reshape(self.width, self.height)\n",
    "        n = grid.shape[0]\n",
    "        visited = np.zeros_like(grid, dtype=bool)\n",
    "        stack = []\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if grid[i, j] > threshold:\n",
    "                    stack.append((i, j))\n",
    "                    break\n",
    "            if stack:\n",
    "                break\n",
    "\n",
    "        while stack:\n",
    "            x, y = stack.pop()\n",
    "            if visited[x, y]:\n",
    "                continue\n",
    "            visited[x, y] = True\n",
    "            for dx, dy in [(-1, 0), (-1, 1), (-1, -1), (1, 0), (1, 1), (1, -1), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < n and 0 <= ny < n and not visited[nx, ny] and grid[nx, ny] > threshold:\n",
    "                    stack.append((nx, ny))\n",
    "        return np.all(visited[grid > threshold])\n",
    "\n",
    "    def find_isolated_cells_class(self, state, width, height, high_threshold=0.7, low_threshold=0.2):\n",
    "        isolated_cells = []\n",
    "        state_np = state.reshape((width, height))\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                if state_np[i, j] > high_threshold:\n",
    "                    is_isolated = True\n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "                            ni, nj = i + di, j + dj\n",
    "                            if 0 <= ni < width and 0 <= nj < width and state_np[ni, nj] > low_threshold:\n",
    "                                is_isolated = False\n",
    "                                break\n",
    "                        if not is_isolated:\n",
    "                            break\n",
    "                    if is_isolated:\n",
    "                        isolated_cells.append(i * width + j)\n",
    "        return isolated_cells\n",
    "\n",
    "    def step(self, action):\n",
    "        cell = int(action // 1)\n",
    "        value = 0.001\n",
    "        self.previous_state = self.state.clone()\n",
    "        reward = 0\n",
    "        self.current_compliance, self.current_constraint = np.random.random(), np.random.random()  \n",
    "        if self.state[cell] > 0.9:\n",
    "            reward += 1\n",
    "        self.state[cell] = torch.tensor(value, dtype=torch.float32, device=self.device)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= 18\n",
    "\n",
    "        reward += self.calculate_reward()\n",
    "        self.reward = reward\n",
    "        truncated = False\n",
    "        return self.state.cpu().numpy(), reward, done, truncated, {} \n",
    "\n",
    "    def calculate_reward(self):\n",
    "        reward = 0\n",
    "        with torch.no_grad():\n",
    "            self.current_compliance, self.current_constraint = optim(args=self.args, x=self.state.cpu().numpy())\n",
    "        reward += 4 / (self.current_compliance) ** 0.5\n",
    "        if self.is_strongly_connected(self.state):\n",
    "            reward += 0.2\n",
    "        if self.is_connected(self.state):\n",
    "            reward += 0.2\n",
    "        isolated_cells = len(self.find_isolated_cells_class(self.state, self.height, self.width))\n",
    "        if isolated_cells > 0:\n",
    "            reward -= 1\n",
    "        self.previous_constraint = self.current_constraint\n",
    "        return reward\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            grid = self.state.cpu().numpy()\n",
    "            grid = np.reshape(grid, (self.height, self.width))\n",
    "            plt.imshow(grid, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "            plt.colorbar()\n",
    "            plt.title(f\"Reward: {self.reward} Compliance: {self.current_compliance:.3f}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "env = BeamOptimizationEnv()\n",
    "\n",
    "def optimize_ppo(trial):\n",
    "    \"\"\"Define the hyperparameter search space for PPO.\"\"\"\n",
    "    return {\n",
    "        'n_steps': trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048]),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1, log=True),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 0.00000001, 0.1, log=True),\n",
    "        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.8, 1.0),\n",
    "        'max_grad_norm': trial.suggest_float('max_grad_norm', 0.3, 5.0),\n",
    "        'vf_coef': trial.suggest_float('vf_coef', 0.1, 1.0),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512]),\n",
    "    }\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function to be minimized.\"\"\"\n",
    "    hyperparameters = optimize_ppo(trial)\n",
    "    model = PPO('MlpPolicy', env, verbose=0, **hyperparameters)\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Hyperparameter optimization ##################################\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    env_name = \"BeamOptimizationEnv\"\n",
    "    has_continuous_action_space = False\n",
    "\n",
    "    max_ep_len = 22\n",
    "    max_training_timesteps = int(2e4)\n",
    "    print_freq = max_ep_len * 4\n",
    "    log_freq = max_ep_len * 2\n",
    "    save_model_freq = int(2e3)\n",
    "\n",
    "    action_std = None\n",
    "    update_timestep = max_ep_len * 4\n",
    "    K_epochs = trial.suggest_int('K_epochs', 68, 78)\n",
    "    eps_clip = trial.suggest_uniform('eps_clip', 0.18, 0.22)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.98, 0.9999)\n",
    "\n",
    "    lr_actor = trial.suggest_loguniform('lr_actor', 1e-5, 1e-3)\n",
    "    lr_critic = trial.suggest_loguniform('lr_critic', 1e-5, 1e-3)\n",
    "\n",
    "    random_seed = 0\n",
    "\n",
    "    print(\"training environment name : \" + env_name)\n",
    "\n",
    "    env = BeamOptimizationEnv()\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    if has_continuous_action_space:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    else:\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "    log_dir = \"PPO_logs\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    log_dir = log_dir + '/' + env_name + '/'\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    run_num = 0\n",
    "    current_num_files = next(os.walk(log_dir))[2]\n",
    "    run_num = len(current_num_files)\n",
    "\n",
    "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "    directory = \"PPO_preTrained\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    directory = directory + '/' + env_name + '/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num)\n",
    "    print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "    log_f = open(log_f_name,\"w+\")\n",
    "    log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "    print_running_reward = 0\n",
    "    print_running_episodes = 0\n",
    "\n",
    "    log_running_reward = 0\n",
    "    log_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "\n",
    "    while time_step <= max_training_timesteps:\n",
    "        state = env.reset()\n",
    "        current_ep_reward = 0\n",
    "\n",
    "        for t in range(1, max_ep_len+1):\n",
    "            action = ppo_agent.select_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "            time_step +=1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo_agent.update()\n",
    "\n",
    "            if time_step % log_freq == 0:\n",
    "                log_avg_reward = log_running_reward / log_running_episodes\n",
    "                log_avg_reward = round(log_avg_reward, 4)\n",
    "                log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "                log_f.flush()\n",
    "\n",
    "                log_running_reward = 0\n",
    "                log_running_episodes = 0\n",
    "\n",
    "            if time_step % print_freq == 0:\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            if time_step % save_model_freq == 0:\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"saving model at : \" + checkpoint_path)\n",
    "                ppo_agent.save(checkpoint_path)\n",
    "                print(\"model saved\")\n",
    "                print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        log_running_reward += current_ep_reward\n",
    "        log_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    log_f.close()\n",
    "    env.close()\n",
    "\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "    print(\"Finished training at (GMT) : \", end_time)\n",
    "    print(\"Total training time  : \", end_time - start_time)\n",
    "\n",
    "    avg_test_reward = print_running_reward / print_running_episodes\n",
    "    return avg_test_reward\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"============================================================================================\")\n",
    "'''\n",
    "Best hyperparameters:  \n",
    "{'K_epochs': 67, 'eps_clip': 0.2500908081350157, 'gamma': 0.9916682057642265, 'lr_actor': 2.4313410999638026e-05, 'lr_critic': 0.00014827591133525898}\n",
    "Best hyperparameters:  \n",
    "{'K_epochs': 74, 'eps_clip': 0.2017951744023234, 'gamma': 0.9903406200399515, 'lr_actor': 9.259619259540716e-05, 'lr_critic': 1.2039904759819646e-05}\n",
    "Best hyperparameters:  \n",
    "{'K_epochs': 77, 'eps_clip': 0.209360498492336, 'gamma': 0.9833317544801661, 'lr_actor': 5.5446905550369185e-05, 'lr_critic': 2.010582676388044e-05}\n",
    "Best hyperparameters:  \n",
    "{'K_epochs': 72, 'eps_clip': 0.20206703270628878, 'gamma': 0.998723559613114, 'lr_actor': 1.1417693510730977e-05, 'lr_critic': 0.0004969498743958001}\n",
    "Best hyperparameters:  \n",
    "{'K_epochs': 73, 'eps_clip': 0.1927592352255087, 'gamma': 0.9908463600626476, 'lr_actor': 0.00013192782890128493, 'lr_critic': 4.012405114153019e-05}\n",
    "\n",
    "'''\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"BeamOptimizationEnv\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 22          # max timesteps in one episode\n",
    "max_training_timesteps = int(3e6)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e3)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "best_params = study.best_params\n",
    "\n",
    "K_epochs = best_params['K_epochs']\n",
    "eps_clip = best_params['eps_clip']\n",
    "gamma = best_params['gamma']\n",
    "lr_actor = best_params['lr_actor']\n",
    "lr_critic = best_params['lr_critic']\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = BeamOptimizationEnv()\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done,truncated, _ = env.step(action)\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "#################################### Testing ###################################\n",
    "\n",
    "\n",
    "################## hyperparameters ##################\n",
    "\n",
    "env_name = \"BeamOptimizationEnv\"\n",
    "has_continuous_action_space = False\n",
    "max_ep_len = 400\n",
    "action_std = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_test_episodes = 10    # total num of testing episodes\n",
    "\n",
    "\n",
    "K_epochs = best_params['K_epochs']\n",
    "eps_clip = best_params['eps_clip']\n",
    "gamma = best_params['gamma']\n",
    "lr_actor = best_params['lr_actor']\n",
    "lr_critic = best_params['lr_critic']\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "env = BeamOptimizationEnv()\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# preTrained weights directory\n",
    "\n",
    "random_seed = 0             \n",
    "run_num_pretrained = 0      \n",
    "\n",
    "\n",
    "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"loading network from : \" + checkpoint_path)\n",
    "\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "test_running_reward = 0\n",
    "\n",
    "for ep in range(1, total_test_episodes+1):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(1, max_ep_len+1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # clear buffer    \n",
    "    ppo_agent.buffer.clear()\n",
    "\n",
    "    test_running_reward +=  ep_reward\n",
    "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
    "    ep_reward = 0\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "avg_test_reward = test_running_reward / total_test_episodes\n",
    "avg_test_reward = round(avg_test_reward, 2)\n",
    "print(\"average test reward : \" + str(avg_test_reward))\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "env_name = 'BeamOptimizationEnv'\n",
    "\n",
    "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
    "\n",
    "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
    "\n",
    "fig_width = 10\n",
    "fig_height = 6\n",
    "\n",
    "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
    "window_len_smooth = 50\n",
    "min_window_len_smooth = 1\n",
    "linewidth_smooth = 1.5\n",
    "alpha_smooth = 1\n",
    "\n",
    "window_len_var = 5\n",
    "min_window_len_var = 1\n",
    "linewidth_var = 2\n",
    "alpha_var = 0.1\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson', 'gray', 'black']\n",
    "\n",
    "# make directory for saving figures\n",
    "figures_dir = \"PPO_figs\"\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "\n",
    "# make environment directory for saving figures\n",
    "figures_dir = figures_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "\n",
    "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
    "\n",
    "# get number of log files in directory\n",
    "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "num_runs = len(current_num_files)\n",
    "\n",
    "all_runs = []\n",
    "\n",
    "for run_num in range(num_runs):\n",
    "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "    print(\"loading data from : \" + log_f_name)\n",
    "    if os.path.getsize(log_f_name) > 0:  \n",
    "        data = pd.read_csv(log_f_name)\n",
    "        data = pd.DataFrame(data)\n",
    "        print(\"data shape : \", data.shape)\n",
    "        all_runs.append(data)\n",
    "    else:\n",
    "        print(f\"File {log_f_name} is empty. Skipping.\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "if plot_avg:\n",
    "    # average all runs\n",
    "    if all_runs:\n",
    "        df_concat = pd.concat(all_runs)\n",
    "        df_concat_groupby = df_concat.groupby(df_concat.index)\n",
    "        data_avg = df_concat_groupby.mean()\n",
    "\n",
    "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
    "        data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
    "        data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
    "\n",
    "        data_avg.plot(kind='line', x='timestep', y='reward_smooth', ax=ax, color=colors[0], linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
    "        data_avg.plot(kind='line', x='timestep', y='reward_var', ax=ax, color=colors[0], linewidth=linewidth_var, alpha=alpha_var)\n",
    "\n",
    "        # keep only reward_smooth in the legend and rename it\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
    "    else:\n",
    "        print(\"No data available to plot.\")\n",
    "else:\n",
    "    for i, run in enumerate(all_runs):\n",
    "        \n",
    "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
    "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
    "\n",
    "\n",
    "        run.plot(kind='line', x='timestep', y='reward_smooth_' + str(i), ax=ax, color=colors[i % len(colors)], linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
    "        run.plot(kind='line', x='timestep', y='reward_var_' + str(i), ax=ax, color=colors[i % len(colors)], linewidth=linewidth_var, alpha=alpha_var)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_handles = []\n",
    "    new_labels = []\n",
    "    for i in range(len(handles)):\n",
    "        if i % 2 == 0:\n",
    "            new_handles.append(handles[i])\n",
    "            new_labels.append(labels[i])\n",
    "    ax.legend(new_handles, new_labels, loc=2)\n",
    "\n",
    "# ax.set_yticks(np.arange(0, 1800, 200))\n",
    "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
    "\n",
    "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
    "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
    "\n",
    "plt.title(env_name, fontsize=14)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(fig_width, fig_height)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "plt.savefig(fig_save_path)\n",
    "print(\"figure saved at : \", fig_save_path)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hyperparameters for inference\n",
    "env_name = \"BeamOptimizationEnv\"\n",
    "has_continuous_action_space = False\n",
    "max_ep_len = 400\n",
    "total_inference_episodes = 10  # Total number of inference episodes\n",
    "\n",
    "# Initialize environment\n",
    "env = BeamOptimizationEnv()\n",
    "\n",
    "# State and action space dimensions\n",
    "state_dim = env.observation_space.shape[0]\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# Initialize PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor=0.0003, lr_critic=0.001, gamma=0.99, K_epochs=80, eps_clip=0.2, has_continuous_action_space=has_continuous_action_space)\n",
    "\n",
    "# Load pre-trained model\n",
    "random_seed = 0             \n",
    "run_num_pretrained = 0     \n",
    "checkpoint_path = f\"PPO_preTrained/{env_name}/PPO_{env_name}_{random_seed}_{run_num_pretrained}.pth\"\n",
    "print(f\"Loading network from: {checkpoint_path}\")\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "test_running_reward = 0\n",
    "for ep in range(1,20):\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len*3 + 1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        if done:\n",
    "            env.render()\n",
    "            break\n",
    "\n",
    "    test_running_reward += ep_reward\n",
    "    print(f'Episode: {ep} \\t\\t Reward: {round(ep_reward, 2)}')\n",
    "\n",
    "avg_test_reward = test_running_reward / total_inference_episodes\n",
    "print(f\"Average test reward: {avg_test_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
