{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihovil/nesto/master_thesis/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "####  IMPORTS\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import time\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box, MultiDiscrete\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nbimporter\n",
    "import autograd, autograd.core, autograd.extend, autograd.tracer  # for adjoints\n",
    "import autograd.numpy as anp      \n",
    "import optuna\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FEM ANALYSIS\n",
    "\n",
    "import time\n",
    "import numpy as np                                                # for dense matrix ops\n",
    "import matplotlib.pyplot as plt                                   # for plotting\n",
    "import autograd, autograd.core, autograd.extend, autograd.tracer  # for adjoints\n",
    "import autograd.numpy as anp      \n",
    "import scipy, scipy.ndimage, scipy.sparse, scipy.sparse.linalg    # sparse matrices\n",
    "import nlopt   \n",
    "\n",
    "\n",
    "def causeway_bridge(width=128, height=128, density=0.08, deck_level=0.2):\n",
    "  \"\"\"A bridge supported by columns at a regular interval.\"\"\"\n",
    "  x_ix, y_ix = 0, 1\n",
    "  normals = np.zeros((width + 1, height + 1, 2))\n",
    "  normals[-1, -1, y_ix] = 1\n",
    "  normals[-1, :, x_ix] = 1\n",
    "  normals[0, :, x_ix] = 1\n",
    "\n",
    "  forces = np.zeros((width + 1, height + 1, 2))\n",
    "  forces[:, round(height * (1 - deck_level)), y_ix] = -1 / width\n",
    "  return normals, forces, density\n",
    "  \n",
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "    \n",
    "def get_args(normals, forces, density=0.4):  \n",
    "  width = normals.shape[0] - 1\n",
    "  height = normals.shape[1] - 1\n",
    "  fixdofs = np.flatnonzero(normals.ravel())\n",
    "  alldofs = np.arange(2 * (width + 1) * (height + 1))\n",
    "  freedofs = np.sort(list(set(alldofs) - set(fixdofs)))\n",
    "  params = {\n",
    "      # material properties\n",
    "      'young': 1, 'young_min': 1e-9, 'poisson': 0.3, 'g': 0,\n",
    "      # constraints\n",
    "      'density': density, 'xmin': 0.001, 'xmax': 1.0,\n",
    "      # input parameters\n",
    "      'nelx': width, 'nely': height, 'mask': 1, 'penal': 3.0, 'filter_width': 1,\n",
    "      'freedofs': freedofs, 'fixdofs': fixdofs, 'forces': forces.ravel(),\n",
    "      # optimization parameters\n",
    "      'opt_steps': 5, 'print_every': 20}\n",
    "  return ObjectView(params)\n",
    "\n",
    "def mbb_beam(width=6, height=6, density=0.4, y=1, x=0):  \n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    normals[-1, -1, y] = 1\n",
    "    normals[0, :, x] = 1\n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    forces[0, width, y] = -1\n",
    "    #forces[0,4,y] = -1\n",
    "    return normals, forces, density\n",
    "#'''\n",
    "def mbb_beam2(width=6, height=6, density=0.4, y=1, x=0):  \n",
    "    normals = np.zeros((width + 1, height + 1, 2))\n",
    "    \n",
    "    normals[0, 0, :] = 1   \n",
    "    normals[0, width, :] = 1  \n",
    "    normals[0, width // 2, :] = 1  \n",
    "    \n",
    "    forces = np.zeros((width + 1, height + 1, 2))\n",
    "    \n",
    "    \n",
    "    forces[-1, :, y] = -1 / width\n",
    "    \n",
    "    return normals, forces, density\n",
    "#'''\n",
    "def _get_solver(a_entries, a_indices, size, sym_pos):\n",
    "  # a is (usu.) symmetric positive; could solve 2x faster w/sksparse.cholmod.cholesky(a).solve_A\n",
    "  a = scipy.sparse.coo_matrix((a_entries, a_indices), shape=(size,)*2).tocsc()\n",
    "  return scipy.sparse.linalg.splu(a).solve\n",
    "\n",
    "@autograd.primitive\n",
    "def solve_coo(a_entries, a_indices, b, sym_pos=False):\n",
    "  solver = _get_solver(a_entries, a_indices, b.size, sym_pos)\n",
    "  return solver(b)\n",
    "\n",
    "def grad_solve_coo_entries(ans, a_entries, a_indices, b, sym_pos=False):\n",
    "  def jvp(grad_ans):\n",
    "    lambda_ = solve_coo(a_entries, a_indices if sym_pos else a_indices[::-1],\n",
    "                        grad_ans, sym_pos)\n",
    "    i, j = a_indices\n",
    "    return -lambda_[i] * ans[j]\n",
    "  return jvp\n",
    "\n",
    "autograd.extend.defvjp(solve_coo, grad_solve_coo_entries,\n",
    "                       lambda: print('err: gradient undefined'),\n",
    "                       lambda: print('err: gradient not implemented'))\n",
    "@autograd.extend.primitive\n",
    "def gaussian_filter(x, width): # 2D gaussian blur/filter\n",
    "  return scipy.ndimage.gaussian_filter(x, width, mode='reflect')\n",
    "\n",
    "def _gaussian_filter_vjp(ans, x, width): # gives the gradient of orig. function w.r.t. x\n",
    "  del ans, x  # unused\n",
    "  return lambda g: gaussian_filter(g, width)\n",
    "autograd.extend.defvjp(gaussian_filter, _gaussian_filter_vjp)\n",
    "def young_modulus(x, e_0, e_min, p=3):\n",
    "  return e_min + x ** p * (e_0 - e_min)\n",
    "\n",
    "def physical_density(x, args, volume_contraint=False, use_filter=False):\n",
    "  x = args.mask * x.reshape(args.nely, args.nelx)  # reshape from 1D to 2D\n",
    "  return gaussian_filter(x, args.filter_width) if use_filter else x  # maybe filter\n",
    "\n",
    "def mean_density(x, args, volume_contraint=False, use_filter=True):\n",
    "  return anp.mean(physical_density(x, args, volume_contraint, use_filter)) / anp.mean(args.mask)\n",
    "def objective_calc(x, args, volume_contraint=False, use_filter=True):\n",
    "  kwargs = dict(penal=args.penal, e_min=args.young_min, e_0=args.young)\n",
    "  x_phys = physical_density(x, args, volume_contraint=volume_contraint, use_filter=use_filter)\n",
    "  ke     = get_stiffness_matrix(args.young, args.poisson)  # stiffness matrix\n",
    "  u      = displace(x_phys, ke, args.forces, args.freedofs, args.fixdofs, **kwargs)\n",
    "  c      = compliance_calc(x_phys, u, ke, **kwargs)\n",
    "  #print(\"x_phys= \",  x_phys)\n",
    "  #print(\"compliance = \", c)\n",
    "  return c\n",
    "def compliance_calc(x_phys, u, ke, *, penal=3, e_min=1e-9, e_0=1):\n",
    "  nely, nelx = x_phys.shape\n",
    "  ely, elx = anp.meshgrid(range(nely), range(nelx))  # x, y coords for the index map\n",
    "\n",
    "  n1 = (nely+1)*(elx+0) + (ely+0)  # nodes\n",
    "  n2 = (nely+1)*(elx+1) + (ely+0)\n",
    "  n3 = (nely+1)*(elx+1) + (ely+1)\n",
    "  n4 = (nely+1)*(elx+0) + (ely+1)\n",
    "  all_ixs = anp.array([2*n1, 2*n1+1, 2*n2, 2*n2+1, 2*n3, 2*n3+1, 2*n4, 2*n4+1])\n",
    "  u_selected = u[all_ixs]  # select from u matrix\n",
    "\n",
    "  ke_u = anp.einsum('ij,jkl->ikl', ke, u_selected)  # compute x^penal * U.T @ ke @ U\n",
    "  ce = anp.einsum('ijk,ijk->jk', u_selected, ke_u)\n",
    "  C = young_modulus(x_phys, e_0, e_min, p=penal) * ce.T\n",
    "  return anp.sum(C)\n",
    "\n",
    "def get_stiffness_matrix(e, nu):  # e=young's modulus, nu=poisson coefficient\n",
    "  k = anp.array([1/2-nu/6, 1/8+nu/8, -1/4-nu/12, -1/8+3*nu/8,\n",
    "                -1/4+nu/12, -1/8-nu/8, nu/6, 1/8-3*nu/8])\n",
    "  return e/(1-nu**2)*anp.array([[k[0], k[1], k[2], k[3], k[4], k[5], k[6], k[7]],\n",
    "                               [k[1], k[0], k[7], k[6], k[5], k[4], k[3], k[2]],\n",
    "                               [k[2], k[7], k[0], k[5], k[6], k[3], k[4], k[1]],\n",
    "                               [k[3], k[6], k[5], k[0], k[7], k[2], k[1], k[4]],\n",
    "                               [k[4], k[5], k[6], k[7], k[0], k[1], k[2], k[3]],\n",
    "                               [k[5], k[4], k[3], k[2], k[1], k[0], k[7], k[6]],\n",
    "                               [k[6], k[3], k[4], k[1], k[2], k[7], k[0], k[5]],\n",
    "                               [k[7], k[2], k[1], k[4], k[3], k[6], k[5], k[0]]])\n",
    "def get_k(stiffness, ke):\n",
    "  # Constructs sparse stiffness matrix k (used in the displace fn)\n",
    "  # First, get position of the nodes of each element in the stiffness matrix\n",
    "  nely, nelx = stiffness.shape\n",
    "  ely, elx = anp.meshgrid(range(nely), range(nelx))  # x, y coords\n",
    "  ely, elx = ely.reshape(-1, 1), elx.reshape(-1, 1)\n",
    "\n",
    "  n1 = (nely+1)*(elx+0) + (ely+0)\n",
    "  n2 = (nely+1)*(elx+1) + (ely+0)\n",
    "  n3 = (nely+1)*(elx+1) + (ely+1)\n",
    "  n4 = (nely+1)*(elx+0) + (ely+1)\n",
    "  edof = anp.array([2*n1, 2*n1+1, 2*n2, 2*n2+1, 2*n3, 2*n3+1, 2*n4, 2*n4+1])\n",
    "  edof = edof.T[0]\n",
    "  x_list = anp.repeat(edof, 8)  # flat list pointer of each node in an element\n",
    "  y_list = anp.tile(edof, 8).flatten()  # flat list pointer of each node in elem\n",
    "\n",
    "  # make the global stiffness matrix K\n",
    "  kd = stiffness.T.reshape(nelx*nely, 1, 1)\n",
    "  value_list = (kd * anp.tile(ke, kd.shape)).flatten()\n",
    "  return value_list, y_list, x_list\n",
    "\n",
    "def displace(x_phys, ke, forces, freedofs, fixdofs, *, penal=3, e_min=1e-9, e_0=1):\n",
    "  # Displaces the load x using finite element techniques (solve_coo=most of runtime)\n",
    "  stiffness = young_modulus(x_phys, e_0, e_min, p=penal)\n",
    "  k_entries, k_ylist, k_xlist = get_k(stiffness, ke)\n",
    "\n",
    "  index_map, keep, indices = _get_dof_indices(freedofs, fixdofs, k_ylist, k_xlist)\n",
    "  \n",
    "  u_nonzero = solve_coo(k_entries[keep], indices, forces[freedofs], sym_pos=True)\n",
    "  u_values = anp.concatenate([u_nonzero, anp.zeros(len(fixdofs))])\n",
    "  return u_values[index_map]\n",
    "def _get_dof_indices(freedofs, fixdofs, k_xlist, k_ylist):\n",
    "  index_map = inverse_permutation(anp.concatenate([freedofs, fixdofs]))\n",
    "  keep = anp.isin(k_xlist, freedofs) & anp.isin(k_ylist, freedofs)\n",
    "  # Now we index an indexing array that is being indexed by the indices of k\n",
    "  i = index_map[k_ylist][keep]\n",
    "  j = index_map[k_xlist][keep]\n",
    "  return index_map, keep, anp.stack([i, j])\n",
    "\n",
    "def inverse_permutation(indices):  # reverses an index operation\n",
    "  inverse_perm = np.zeros(len(indices), dtype=anp.int64)\n",
    "  inverse_perm[indices] = np.arange(len(indices), dtype=anp.int64)\n",
    "  return inverse_perm\n",
    "def fast_stopt(args, x=None, verbose=True):\n",
    "  if x is None:\n",
    "    x = anp.ones((args.nely, args.nelx)) * args.density  # init mass\n",
    "\n",
    "  reshape = lambda x: x.reshape(args.nely, args.nelx)\n",
    "  objective_fn = lambda x: objective_calc(reshape(x), args) # don't enforce mass constraint here\n",
    "  constraint = lambda params: mean_density(reshape(params), args) - args.density\n",
    "\n",
    "  def wrap_autograd_func(func, losses=None, frames=None):\n",
    "    def wrapper(x, grad):\n",
    "      if grad.size > 0:\n",
    "        value, grad[:] = autograd.value_and_grad(func)(x)\n",
    "      else:\n",
    "        value = func(x)\n",
    "      if losses is not None:\n",
    "        losses.append(value)\n",
    "      if frames is not None:\n",
    "        frames.append(reshape(x).copy())\n",
    "        if verbose and len(frames) % args.print_every == 0:\n",
    "          print('step {}, loss {:.2e}, t={:.2f}s'.format(len(frames), value, time.time()-dt))\n",
    "      return value\n",
    "    return wrapper\n",
    "\n",
    "  losses, frames = [], [] ; dt = time.time()\n",
    "  print('Optimizing a problem with {} nodes'.format(len(args.forces)))\n",
    "  opt = nlopt.opt(nlopt.LD_MMA, x.size)\n",
    "  opt.set_lower_bounds(0.0) ; opt.set_upper_bounds(1.0)\n",
    "  opt.set_min_objective(wrap_autograd_func(objective_fn, losses, frames))\n",
    "\n",
    "  opt.add_inequality_constraint(wrap_autograd_func(constraint), 1e-8)\n",
    "  opt.set_maxeval(args.opt_steps + 1)\n",
    "  opt.optimize(x.flatten())\n",
    "  return np.array(losses), reshape(frames[-1]), np.array(frames), constraint(frames[-1])\n",
    "  #return objective(x, args), constraint(x)  #losses[-1]\n",
    "\n",
    "def optim( args, x=None, verbose = True):\n",
    "    if x is None:\n",
    "        x = anp.ones((args.nely, args.nelx)) * args.density  # init mass\n",
    "    reshape = lambda x: x.reshape(args.nely, args.nelx)\n",
    "    objective_fn = lambda x: objective_calc(reshape(x), args) # don't enforce mass constraint here\n",
    "    constraint = lambda params: mean_density(reshape(params), args) \n",
    "    return objective_calc(x, args), constraint(x)  #losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model and environment state.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, save_path: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Save the model\n",
    "            model_path = os.path.join(self.save_path, 'model.zip')\n",
    "            self.model.save(model_path)\n",
    "\n",
    "            # Save the environment state\n",
    "            env_path = os.path.join(self.save_path, 'env_state.npy')\n",
    "            env_state = self.training_env.get_attr('state')\n",
    "            env_state_cpu = [state.cpu().numpy() for state in env_state]  # Move to CPU and convert to numpy\n",
    "            np.save(env_path, env_state_cpu)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Saving model checkpoint to {model_path}\")\n",
    "                print(f\"Saving environment state to {env_path}\")\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardLoggingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for logging episode rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_path, verbose=1):\n",
    "        super(RewardLoggingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = []\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.locals['dones'][0]:\n",
    "            episode_reward = np.sum(self.current_rewards)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.current_rewards = []\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Episode reward: {episode_reward}\")\n",
    "        else:\n",
    "            self.current_rewards.append(self.locals['rewards'][0])\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Save the rewards to a file\n",
    "        np.save(self.save_path, np.array(self.episode_rewards))\n",
    "        if self.verbose > 0:\n",
    "            print(\"Training finished\")\n",
    "            print(f\"Episode rewards: {self.episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihovil/nesto/master_thesis/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import optuna\n",
    "\n",
    "class BeamOptimizationEnv(gym.Env):\n",
    "    def __init__(self, width=6, height=6, density=0.4):\n",
    "        super(BeamOptimizationEnv, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.density = density\n",
    "        self.max_steps = self.width * self.height\n",
    "        self.action_space = gym.spaces.Discrete(self.width * self.height)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(self.width * self.height,), dtype=np.float32)\n",
    "        self.previous_constraint = 1.0\n",
    "        self.significant_improvement = 10\n",
    "        self.state = torch.ones((self.height, self.width), dtype=torch.float32, device=self.device) \n",
    "        self.visited = torch.zeros((self.height * self.width), dtype=bool, device=self.device)\n",
    "        self.visited_cells = set()\n",
    "        normals, forces, _ = mbb_beam(width, height, density)\n",
    "        self.normals = torch.tensor(normals, dtype=torch.float32, device=self.device)\n",
    "        self.forces = torch.tensor(forces, dtype=torch.float32, device=self.device)\n",
    "        self.args = get_args(self.normals.cpu().numpy(), self.forces.cpu().numpy(), density)\n",
    "        self.current_compliance = float('inf')\n",
    "        self.previous_compliance = 10\n",
    "        self.current_step = 0\n",
    "        self.reward = 0\n",
    "        self.current_mode = 'mode1'\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        self.state = torch.ones((self.width * self.height), dtype=torch.float32, device=self.device) \n",
    "        self.current_step = 0\n",
    "        self.visited = torch.zeros((self.width * self.height), dtype=bool)\n",
    "        self.visited_cells = set()\n",
    "        self.current_compliance = float('inf')\n",
    "        self.previous_compliance = 10\n",
    "        self.reward = 0\n",
    "        return self.state.cpu().numpy(), {}\n",
    "\n",
    "    def is_strongly_connected(self, state, threshold=0.8):\n",
    "        grid = state.cpu().numpy().reshape(self.height, self.width)\n",
    "        for i in range(self.height - 1):\n",
    "            for j in range(self.width - 1):\n",
    "                if grid[i, j] > threshold and grid[i + 1, j + 1] > threshold:\n",
    "                    if grid[i, j + 1] <= threshold and grid[i + 1, j] <= threshold:\n",
    "                        return False\n",
    "                if grid[i + 1, j] > threshold and grid[i, j + 1] > threshold:\n",
    "                    if grid[i, j] <= threshold and grid[i + 1, j + 1] <= threshold:\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def is_connected(self, state, threshold=0.8):\n",
    "        grid = state.cpu().numpy().reshape(self.width, self.height)\n",
    "        n = grid.shape[0]\n",
    "        visited = np.zeros_like(grid, dtype=bool)\n",
    "        stack = []\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if grid[i, j] > threshold:\n",
    "                    stack.append((i, j))\n",
    "                    break\n",
    "            if stack:\n",
    "                break\n",
    "\n",
    "        while stack:\n",
    "            x, y = stack.pop()\n",
    "            if visited[x, y]:\n",
    "                continue\n",
    "            visited[x, y] = True\n",
    "            for dx, dy in [(-1, 0), (-1, 1), (-1, -1), (1, 0), (1, 1), (1, -1), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < n and 0 <= ny < n and not visited[nx, ny] and grid[nx, ny] > threshold:\n",
    "                    stack.append((nx, ny))\n",
    "        return np.all(visited[grid > threshold])\n",
    "\n",
    "    def find_isolated_cells_class(self, state, width, height, high_threshold=0.7, low_threshold=0.2):\n",
    "        isolated_cells = []\n",
    "        state_np = state.reshape((width, height))\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                if state_np[i, j] > high_threshold:\n",
    "                    is_isolated = True\n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "                            ni, nj = i + di, j + dj\n",
    "                            if 0 <= ni < width and 0 <= nj < width and state_np[ni, nj] > low_threshold:\n",
    "                                is_isolated = False\n",
    "                                break\n",
    "                        if not is_isolated:\n",
    "                            break\n",
    "                    if is_isolated:\n",
    "                        isolated_cells.append(i * width + j)\n",
    "        return isolated_cells\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_mode == 'mode1':\n",
    "            return self.step1(action)\n",
    "        elif self.current_mode == 'mode2':\n",
    "            return self.step2(action)\n",
    "        else:\n",
    "            return self.step3(action)\n",
    "\n",
    "    def step1(self, action):\n",
    "        cell = int(action // 1)\n",
    "        value = 0.001\n",
    "        self.previous_state = self.state.clone()\n",
    "        reward = 0\n",
    "        with torch.no_grad():\n",
    "            self.current_compliance, self.current_constraint = optim(args=self.args, x=self.state.cpu().numpy())\n",
    "        if self.state[cell] > 0.9:\n",
    "            reward += 1\n",
    "        self.state[cell] = torch.tensor(value, dtype=torch.float32, device=self.device)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= 9\n",
    "\n",
    "        reward += self.calculate_reward()\n",
    "        self.reward = reward\n",
    "        truncated = False\n",
    "        return self.state.cpu().numpy(), reward, done, truncated, {}\n",
    "    \n",
    "    def step2(self, action):\n",
    "        cell = int(action // 1)\n",
    "        self.previous_state = self.state.clone()\n",
    "        reward = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.current_compliance, self.current_constraint = optim(args=self.args, x=self.state.cpu().numpy())\n",
    "        \n",
    "        if self.state[cell] < 0.1:\n",
    "            reward += 1\n",
    "        value = 0.999\n",
    "\n",
    "        #reward za popravak compliancea\n",
    "        \n",
    "        self.state[cell] = torch.tensor(value, dtype=torch.float32, device=self.device)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= 6\n",
    "\n",
    "        reward += self.calculate_reward()\n",
    "        self.reward = reward\n",
    "        truncated = False\n",
    "        \n",
    "        return self.state.cpu().numpy(), reward, done, truncated, {}\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        reward = 0\n",
    "        with torch.no_grad():\n",
    "            self.current_compliance, self.current_constraint = optim(args=self.args, x=self.state.cpu().numpy())\n",
    "        reward += 1 / (self.current_compliance) ** 0.5\n",
    "        if self.is_strongly_connected(self.state):\n",
    "            reward += 0.1\n",
    "        if self.is_connected(self.state):\n",
    "            reward += 0.2\n",
    "        isolated_cells = len(self.find_isolated_cells_class(self.state, self.height, self.width))\n",
    "        if isolated_cells > 0:\n",
    "            reward -= 1\n",
    "        self.previous_constraint = self.current_constraint\n",
    "        return reward\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            grid = self.state.cpu().numpy()\n",
    "            grid = np.reshape(grid, (self.height, self.width))\n",
    "            plt.imshow(grid, cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "            plt.colorbar()\n",
    "            plt.title(f\"Reward: {self.reward} Compliance: {self.current_compliance:.3f}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "env = BeamOptimizationEnv()\n",
    "check_env(env)\n",
    "\n",
    "def optimize_ppo(trial):\n",
    "    \"\"\"Define the hyperparameter search space for PPO.\"\"\"\n",
    "    return {\n",
    "        'n_steps': trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048]),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1, log=True),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 0.00000001, 0.1, log=True),\n",
    "        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.8, 1.0),\n",
    "        'max_grad_norm': trial.suggest_float('max_grad_norm', 0.3, 5.0),\n",
    "        'vf_coef': trial.suggest_float('vf_coef', 0.1, 1.0),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512]),\n",
    "    }\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function to be minimized.\"\"\"\n",
    "    hyperparameters = optimize_ppo(trial)\n",
    "    model = PPO('MlpPolicy', env, verbose=0, **hyperparameters)\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "#study = optuna.create_study(direction='maximize')\n",
    "#study.optimize(objective, n_trials=100, n_jobs=1)\n",
    "\n",
    "#print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "best_hyperparams = {'n_steps': 1024, 'gamma': 0.9268317653886465, 'learning_rate': 0.0008844770142722593, \n",
    "'ent_coef': 4.0387142903583754e-08, 'clip_range': 0.21038826076990821, 'gae_lambda': 0.8353105326939684,\n",
    " 'max_grad_norm': 3.6175740624337367, 'vf_coef': 0.6852703238198472, 'batch_size': 128}\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks and save path\n",
    "save_path_1 = './ppo_checkpoints_first/'\n",
    "model_path_1 = os.path.join(save_path_1, 'model.zip')\n",
    "env_state_path_1 = os.path.join(save_path_1, 'env_state.npy')\n",
    "episode_rewards_path_1 = os.path.join(save_path_1, 'episode_rewards_1.npy')\n",
    "\n",
    "callback_1 = SaveOnBestTrainingRewardCallback(check_freq=1000, save_path=save_path_1)\n",
    "reward_logging_callback_1 = RewardLoggingCallback(save_path=episode_rewards_path_1, verbose=1)\n",
    "#callback_list_1 = CallbackList([callback_1, reward_logging_callback_1])\n",
    "\n",
    "# Train the first PPO model\n",
    "model_1 = PPO('MlpPolicy', env, verbose=1, **best_hyperparams)\n",
    "model_1.learn(total_timesteps=1_00_000, callback=callback_1)\n",
    "\n",
    "# Save the model and environment state\n",
    "model_1.save(model_path_1)\n",
    "env_state = env.state.cpu().numpy()\n",
    "np.save(env_state_path_1, env_state)\n",
    "\n",
    "# Load the first model and state\n",
    "model_1 = PPO.load(model_path_1, env=env)\n",
    "env_state = np.load(env_state_path_1, allow_pickle=True)\n",
    "env.reset()\n",
    "env.state = torch.tensor(env_state, dtype=torch.float32, device=env.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "CURRENT STEP IS: 5\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9        |\n",
      "|    ep_rew_mean     | 12.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 140      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010125389 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.58       |\n",
      "|    explained_variance   | 0.0169      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 8.22        |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012892703 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.56       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 5.57        |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9            |\n",
      "|    ep_rew_mean          | 12.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097988155 |\n",
      "|    clip_fraction        | 0.0799       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.56        |\n",
      "|    explained_variance   | 0.604        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0217      |\n",
      "|    value_loss           | 3.57         |\n",
      "------------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9          |\n",
      "|    ep_rew_mean          | 12.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 129        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00967052 |\n",
      "|    clip_fraction        | 0.0812     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.56      |\n",
      "|    explained_variance   | 0.79       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.347      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 1.91       |\n",
      "----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009907804 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.55       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.195       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.906       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009295022 |\n",
      "|    clip_fraction        | 0.078       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.55       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.208       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.781       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010214168 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.54       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.333       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.787       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053759 |\n",
      "|    clip_fraction        | 0.0943      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.54       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.201       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.829       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008965269 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.53       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.185       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.658       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009809103 |\n",
      "|    clip_fraction        | 0.0779      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.53       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.097       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.645       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009986125 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.52       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.732       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010295425 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.52       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.229       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.648       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012512422 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.5        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    value_loss           | 0.819       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009907609 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.49       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.56        |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009179528 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.47       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.118       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.482       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011527523 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.46       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.548       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009431725 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.45       |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    value_loss           | 0.549       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 13          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011883006 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.44       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.483       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 12.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009470876 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.42       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0806      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.549       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 13          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011428829 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.41       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    value_loss           | 0.491       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 13          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011595879 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.41       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.409       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 13.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012962189 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.39       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0767      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    value_loss           | 0.407       |\n",
      "-----------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9            |\n",
      "|    ep_rew_mean          | 12.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 387          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111451605 |\n",
      "|    clip_fraction        | 0.142        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.38        |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0355       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0406      |\n",
      "|    value_loss           | 0.399        |\n",
      "------------------------------------------\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "Saving model checkpoint to ./ppo_checkpoints_second/model.zip\n",
      "Saving environment state to ./ppo_checkpoints_second/env_state.npy\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 13          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012811273 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.36       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0251      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.382       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7408219dfee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the second PPO model from the state of the first model\n",
    "save_path_2 = './ppo_checkpoints_second/'\n",
    "model_path_2 = os.path.join(save_path_2, 'model.zip')\n",
    "env_state_path_2 = os.path.join(save_path_2, 'env_state.npy')\n",
    "episode_rewards_path_2 = os.path.join(save_path_2, 'episode_rewards_2.npy')\n",
    "\n",
    "callback_2 = SaveOnBestTrainingRewardCallback(check_freq=1000, save_path=save_path_2)\n",
    "reward_logging_callback_2 = RewardLoggingCallback(save_path=episode_rewards_path_2, verbose=1)\n",
    "#callback_list_2 = CallbackList([callback_3, reward_logging_callback_2])\n",
    "\n",
    "new_hyperparams = {\n",
    "    'ent_coef': 0.02,  # Increase entropy coefficient to encourage exploration\n",
    "    # 'learning_rate': 1e-4,  # Adjust learning rate if needed\n",
    "    # Add any other hyperparameters adjustments here\n",
    "}\n",
    "\n",
    "model_2 = PPO('MlpPolicy', env, verbose=1, **new_hyperparams)\n",
    "\n",
    "env.current_step = 5  # Uncomment if needed\n",
    "print(f\"CURRENT STEP IS: {env.current_step}\")\n",
    "\n",
    "model_2.learn(total_timesteps=5_00_00, callback=callback_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the third PPO model from the state of the previous model\n",
    "save_path_3 = './ppo_checkpoints_third/'\n",
    "model_path_3 = os.path.join(save_path_3, 'model.zip')\n",
    "env_state_path_3 = os.path.join(save_path_3, 'env_state.npy')\n",
    "episode_rewards_path_3 = os.path.join(save_path_3, 'episode_rewards_3.npy')\n",
    "\n",
    "callback_3 = SaveOnBestTrainingRewardCallback(check_freq=1000, save_path=save_path_3)\n",
    "reward_logging_callback_3 = RewardLoggingCallback(save_path=episode_rewards_path_3, verbose=1)\n",
    "#callback_list_3 = CallbackList([callback_4, reward_logging_callback_3])\n",
    "\n",
    "new_hyperparams = {\n",
    "    'ent_coef': 0.03,  \n",
    "    # 'learning_rate': 1e-4,  \n",
    "}\n",
    "\n",
    "model_3 = PPO('MlpPolicy', env, verbose=1, **new_hyperparams)\n",
    "\n",
    "env.current_step = 6  # Uncomment if needed\n",
    "print(f\"CURRENT STEP IS: {env.current_step}\")\n",
    "\n",
    "model_3.learn(total_timesteps=5_00_00, callback=callback_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the fourth PPO model from the state of the third model\n",
    "save_path_4 = './ppo_checkpoints_fourth/'\n",
    "model_path_4 = os.path.join(save_path_4, 'model.zip')\n",
    "env_state_path_4 = os.path.join(save_path_4, 'env_state.npy')\n",
    "episode_rewards_path_4 = os.path.join(save_path_4, 'episode_rewards_4.npy')\n",
    "\n",
    "callback_4 = SaveOnBestTrainingRewardCallback(check_freq=1000, save_path=save_path_4)\n",
    "reward_logging_callback_4 = RewardLoggingCallback(save_path=episode_rewards_path_4, verbose=1)\n",
    "#callback_list_4 = CallbackList([callback_4, reward_logging_callback_4])\n",
    "\n",
    "new_hyperparams = {\n",
    "    'ent_coef': 0.03,  # Increase entropy coefficient to encourage exploration\n",
    "    # 'learning_rate': 1e-4,  # Adjust learning rate if needed\n",
    "    # Add any other hyperparameters adjustments here\n",
    "}\n",
    "\n",
    "env.current_mode = 'mode2'\n",
    "env.current_step = 0\n",
    "\n",
    "model_4 = PPO('MlpPolicy', env, verbose=1, **new_hyperparams)\n",
    "\n",
    "env.current_mode = \"mode2\"  # Uncomment if needed\n",
    "print(f\"CURRENT STEP IS: {env.current_step}\")\n",
    "\n",
    "model_4.learn(total_timesteps=5_00_00, callback=callback_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import optuna, os\n",
    "\n",
    "save_path_1 = './ppo_checkpoints_first/'\n",
    "save_path_2 = './ppo_checkpoints_second/'\n",
    "save_path_3 = './ppo_checkpoints_third/'\n",
    "save_path_4 = './ppo_checkpoints_fourth/'\n",
    "\n",
    "# Load the trained models\n",
    "model_1 = PPO.load(os.path.join(save_path_1, 'model.zip'), env=env)\n",
    "model_2 = PPO.load(os.path.join(save_path_2, 'model.zip'), env=env)\n",
    "model_3 = PPO.load(os.path.join(save_path_3, 'model.zip'), env=env)\n",
    "model_4 = PPO.load(os.path.join(save_path_4, 'model.zip'), env=env)\n",
    "\n",
    "\n",
    "env.current_mode = 'mode1'\n",
    "# Load the saved environment state\n",
    "env_state = np.load(os.path.join(save_path_1, 'env_state.npy'), allow_pickle=True)\n",
    "\n",
    "# Reset the environment and set the state\n",
    "env.reset()\n",
    "env.state = torch.tensor(env_state[0], dtype=torch.float32, device=env.device)\n",
    "\n",
    "# Inference with the first model\n",
    "obs, _ = env.reset()\n",
    "dones = False\n",
    "print(\"Inference with Model 1:\")\n",
    "while not dones:\n",
    "    action, _states = model_1.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Capture the final state after the first model's inference\n",
    "final_state_after_model_1 = env.state.clone()\n",
    "\n",
    "env.current_step = 4\n",
    "env.current_mode = 'mode1'\n",
    "# Inference with the second model starting from the final state of the first model\n",
    "env.state = final_state_after_model_1  \n",
    "obs = final_state_after_model_1.cpu().numpy().flatten() \n",
    "dones = False\n",
    "print(\"Inference with Model 2:\")\n",
    "i=0\n",
    "while not dones:\n",
    "    action, _states = model_2.predict(obs, deterministic=False)\n",
    "    obs, rewards, dones, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    #print(f\"Model 2 - State: {obs}, Action: {action}, Reward: {rewards}\")\n",
    "\n",
    "    i+=1\n",
    "\n",
    "final_state_after_model_2 = env.state.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "env.current_step = 5\n",
    "env.current_mode = 'mode1'\n",
    "\n",
    "# Inference with the third model starting from the final state of the second model\n",
    "env.state = final_state_after_model_2  \n",
    "obs = final_state_after_model_1.cpu().numpy().flatten()  \n",
    "dones = False\n",
    "print(\"Inference with Model 3:\")\n",
    "i=0\n",
    "while not dones:\n",
    "    action, _states = model_3.predict(obs, deterministic=False)\n",
    "    obs, rewards, dones, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    #print(f\"Model 3 - State: {obs}, Action: {action}, Reward: {rewards}\")\n",
    "\n",
    "    i+=1\n",
    "\n",
    "final_state_after_model_3 = env.state.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_state_after_model_3 = env.state.clone()\n",
    "\n",
    "\n",
    "env.current_mode = 'mode2'\n",
    "env.current_step = 0\n",
    "# Inference with the fourth model starting from the final state of the third model\n",
    "env.state = final_state_after_model_3  \n",
    "obs = final_state_after_model_3.cpu().numpy().flatten()  \n",
    "dones = False\n",
    "print(\"Inference with Model 4:\")\n",
    "i=0\n",
    "while not dones:\n",
    "    action, _states = model_4.predict(obs, deterministic=False)\n",
    "    obs, rewards, dones, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Model 4 - State: {obs}, Action: {action}, Reward: {rewards}\")\n",
    "\n",
    "    i+=1\n",
    "\n",
    "final_state_after_model_4 = env.state.clone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
